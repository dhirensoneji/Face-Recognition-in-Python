{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1- Collecting Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Samples for Dhiren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Collecting Samples for Dhiren Complete\n"
     ]
    }
   ],
   "source": [
    "#Sample Collection for Dhiren\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Save file in specified directory with unique name\n",
    "        file_name_path = './faces/dhiren/' + str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples for Dhiren Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting samples for Vaidik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Collecting Samples for Vaidik Complete\n"
     ]
    }
   ],
   "source": [
    "#Sample collection for Vaidik\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Save file in specified directory with unique name\n",
    "        file_name_path = './faces/vaidik/' + str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples for Vaidik Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model for Dhiren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained sucessefully for Dhiren\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Get the training data we previously made\n",
    "data_path = './faces/dhiren/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "# Create arrays for training data and labels\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "# Open training images in our datapath\n",
    "# Create a numpy array for training data\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "\n",
    "# Create a numpy array for both training data and labels\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "# Initialize facial recognizer\n",
    "# model = cv2.face.createLBPHFaceRecognizer()\n",
    "# NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "# pip install opencv-contrib-python\n",
    "# model = cv2.createLBPHFaceRecognizer()\n",
    "\n",
    "#vimal_model  = cv2.face.LBPHFaceRecognizer.create()\n",
    "dhiren_model = cv2.face.LBPHFaceRecognizer_create()\n",
    "# Let's train our model \n",
    "dhiren_model.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "print(\"Model trained sucessefully for Dhiren\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for Vaidik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained sucessefully for Vaidik\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Get the training data we previously made\n",
    "data_path = './faces/vaidik/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "# Create arrays for training data and labels\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "# Open training images in our datapath\n",
    "# Create a numpy array for training data\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "\n",
    "# Create a numpy array for both training data and labels\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "# Initialize facial recognizer\n",
    "# model = cv2.face.createLBPHFaceRecognizer()\n",
    "# NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "# pip install opencv-contrib-python\n",
    "# model = cv2.createLBPHFaceRecognizer()\n",
    "\n",
    "#vimal_model  = cv2.face.LBPHFaceRecognizer.create()\n",
    "vaidik_model = cv2.face.LBPHFaceRecognizer_create()\n",
    "# Let's train our model \n",
    "vaidik_model.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "print(\"Model trained sucessefully for Vaidik\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Run Our Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 30 seconds web.whatsapp.com will open and after 5 seconds message will be delivered\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "from playsound import playsound\n",
    "import pywhatkit\n",
    "import time\n",
    "\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "fromaddr = \"\"\n",
    "toaddr = \"\"\n",
    "\n",
    "# instance of MIMEMultipart\n",
    "msg = MIMEMultipart()\n",
    "\n",
    "# storing the senders email address\n",
    "msg['From'] = fromaddr\n",
    "\n",
    "# storing the receivers email address\n",
    "msg['To'] = toaddr\n",
    "\n",
    "# storing the subject\n",
    "msg['Subject'] = \"Person Detected\"\n",
    "\n",
    "# string to store the body of the mail\n",
    "body = \"This is the face of Dhiren\"\n",
    "\n",
    "def face_detector(img, size=0.5):\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return img, []\n",
    "    \n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    return img, roi\n",
    "\n",
    "\n",
    "# Open Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "flag=True\n",
    "flag1=True\n",
    "count=0\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    image, face = face_detector(frame)\n",
    "    \n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Pass face to prediction model\n",
    "        # \"results\" comprises of a tuple containing the label and the confidence value\n",
    "        dhiren_results = dhiren_model.predict(face)\n",
    "        vaidik_results = vaidik_model.predict(face)\n",
    "        # harry_model.predict(face)\n",
    "        \n",
    "\n",
    "        dhiren_confidence = int( 100 * (1 - (dhiren_results[1])/400) )\n",
    "        vaidik_confidence = int( 100 * (1 - (vaidik_results[1])/400) )\n",
    "        \n",
    "        if dhiren_confidence > 90:\n",
    "            count += 1\n",
    "            display_string_dhiren = str(dhiren_confidence) + '% Confident'\n",
    "            cv2.putText(image, display_string_dhiren, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2)\n",
    "            cv2.putText(image, \"Hey Dhiren\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "            cv2.imwrite(\"Detected_face.jpg\",image)\n",
    "            #os.system(\"chrome https://www.google.com/search?q=vimal+daga\")\n",
    "            #os.system(\"wmplayer   c:\\lw.mp3\")\n",
    "            if flag == True and count==10:\n",
    "                playsound(\"HeyDhiren.mp3\",0)\n",
    "                msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "                # open the file to be sent\n",
    "                filename = \"Detected_face.jpg\"\n",
    "                attachment = open(\"./Detected_face.jpg\", \"rb\")\n",
    "\n",
    "                # instance of MIMEBase and named as p\n",
    "                p = MIMEBase('application', 'octet-stream')\n",
    "\n",
    "                # To change the payload into encoded form\n",
    "                p.set_payload((attachment).read())\n",
    "\n",
    "                # encode into base64\n",
    "                encoders.encode_base64(p)\n",
    "\n",
    "                p.add_header('Content-Disposition', \"attachment; filename= %s\" % filename)\n",
    "\n",
    "                # attach the instance 'p' to instance 'msg'\n",
    "                msg.attach(p)\n",
    "\n",
    "                # creates SMTP session\n",
    "                s = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "\n",
    "                # start TLS for security\n",
    "                s.starttls()\n",
    "\n",
    "                # Authentication\n",
    "                s.login(fromaddr, \"Password\")\n",
    "\n",
    "                # Converts the Multipart msg into a string\n",
    "                text = msg.as_string()\n",
    "\n",
    "                # sending the mail\n",
    "                s.sendmail(fromaddr, toaddr, text)\n",
    "\n",
    "                # terminating the session\n",
    "                s.quit()\n",
    "                playsound(\"MailSent.mp3\",0)\n",
    "                myTime = time.strftime('%H:%M:%S')\n",
    "                pywhatkit.sendwhatmsg('+918200801152', 'Face Detected', int(myTime[:2]),int(myTime[3:5])+1,5)\n",
    "\n",
    "                flag = False\n",
    "        elif vaidik_confidence > 90:\n",
    "            display_string_vaidik = str(vaidik_confidence) + '% Confident'\n",
    "            cv2.putText(image, \"Hey Vaidik\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.putText(image, display_string_vaidik, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "            if flag1 == True:\n",
    "                playsound(\"HeyVaidik.mp3\",0)\n",
    "                inst = !aws ec2 run-instances --image-id ami-0ad704c126371a549 --instance-type t2.micro --subnet-id subnet-29e31642\n",
    "                inst_id = inst[6][27:-2]\n",
    "                create_vol = !aws ec2 create-volume --volume-type gp2 --size 5 --region ap-south-1 --availability-zone ap-south-1a\n",
    "                vol_id = create_vol[7][17:-2]\n",
    "                playsound(\"AWSInst.mp3\",0)\n",
    "                time.sleep(20)\n",
    "                attach_vol=!aws ec2 attach-volume --volume-id $vol_id --instance-id $inst_id --device /dev/sdf --region ap-south-1\n",
    "                playsound(\"VolAttach.mp3\",0)\n",
    "                flag1=False\n",
    "        else:\n",
    "            cv2.putText(image, \"I dont know, who are you\", (150, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "\n",
    "    except:\n",
    "        cv2.putText(image, \"No Face Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"looking for face\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        pass\n",
    "        \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
